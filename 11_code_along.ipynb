{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vanishing / Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different weight initialization technique (default=\"glorot_uniform\")\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement any initialization technique with VarianceScaling\n",
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n",
    "                                                    distribution=\"uniform\")\n",
    "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LealyReLU\n",
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 11:30:07.669009: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-04-08 11:30:07.669037: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-04-08 11:30:07.669050: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-04-08 11:30:07.669100: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-08 11:30:07.669122: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# LealyReLU as a separate layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"), # no activation\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2)  # activation as a separate layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BN layer before the activation function\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    # we can omit the bias term since BN also has one\n",
    "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 16:02:24.973818: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-04-08 16:02:24.990725: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376/1376 [==============================] - 8s 5ms/step - loss: 1.0495 - accuracy: 0.6385 - val_loss: 0.6816 - val_accuracy: 0.7593\n",
      "Epoch 2/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.6018 - accuracy: 0.7945 - val_loss: 0.5353 - val_accuracy: 0.8132\n",
      "Epoch 3/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.5039 - accuracy: 0.8324 - val_loss: 0.4732 - val_accuracy: 0.8318\n",
      "Epoch 4/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.4523 - accuracy: 0.8483 - val_loss: 0.4325 - val_accuracy: 0.8508\n",
      "Epoch 5/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.4187 - accuracy: 0.8584 - val_loss: 0.4060 - val_accuracy: 0.8584\n",
      "Epoch 6/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3945 - accuracy: 0.8665 - val_loss: 0.3867 - val_accuracy: 0.8659\n",
      "Epoch 7/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3769 - accuracy: 0.8717 - val_loss: 0.3702 - val_accuracy: 0.8696\n",
      "Epoch 8/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3628 - accuracy: 0.8767 - val_loss: 0.3640 - val_accuracy: 0.8742\n",
      "Epoch 9/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3522 - accuracy: 0.8799 - val_loss: 0.3509 - val_accuracy: 0.8767\n",
      "Epoch 10/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3427 - accuracy: 0.8831 - val_loss: 0.3433 - val_accuracy: 0.8787\n",
      "Epoch 11/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3349 - accuracy: 0.8852 - val_loss: 0.3423 - val_accuracy: 0.8807\n",
      "Epoch 12/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3285 - accuracy: 0.8877 - val_loss: 0.3384 - val_accuracy: 0.8807\n",
      "Epoch 13/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3236 - accuracy: 0.8895 - val_loss: 0.3277 - val_accuracy: 0.8839\n",
      "Epoch 14/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3183 - accuracy: 0.8903 - val_loss: 0.3219 - val_accuracy: 0.8864\n",
      "Epoch 15/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3142 - accuracy: 0.8916 - val_loss: 0.3186 - val_accuracy: 0.8887\n",
      "Epoch 16/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3103 - accuracy: 0.8934 - val_loss: 0.3173 - val_accuracy: 0.8862\n",
      "Epoch 17/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3069 - accuracy: 0.8936 - val_loss: 0.3144 - val_accuracy: 0.8892\n",
      "Epoch 18/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3033 - accuracy: 0.8951 - val_loss: 0.3141 - val_accuracy: 0.8882\n",
      "Epoch 19/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.3005 - accuracy: 0.8975 - val_loss: 0.3089 - val_accuracy: 0.8884\n",
      "Epoch 20/20\n",
      "1376/1376 [==============================] - 6s 4ms/step - loss: 0.2977 - accuracy: 0.8973 - val_loss: 0.3100 - val_accuracy: 0.8910\n",
      "INFO:tensorflow:Assets written to: my_model_A/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_A/assets\n"
     ]
    }
   ],
   "source": [
    "# extra code – split Fashion MNIST into tasks A and B, then train and save\n",
    "#              model A to \"my_model_A\".\n",
    "\n",
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)  # classes 0 and 2\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)  # 0 becomes False and then 0., 2 becomes True and then 1.\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, ..., False,  True,  True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_for_B = (y_train == pos_class_id) | (y_train == neg_class_id)\n",
    "y_for_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 2], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_for_B] == pos_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train[y_for_B] == pos_class_id).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 16:43:18.634324: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 201ms/step - loss: 0.7871 - accuracy: 0.4650 - val_loss: 0.7668 - val_accuracy: 0.4491\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.7340 - accuracy: 0.5100 - val_loss: 0.7194 - val_accuracy: 0.5163\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.6867 - accuracy: 0.5650 - val_loss: 0.6776 - val_accuracy: 0.5826\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6373 - accuracy: 0.6150 - val_loss: 0.6393 - val_accuracy: 0.6400\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.5991 - accuracy: 0.6900 - val_loss: 0.6021 - val_accuracy: 0.6864\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5612 - accuracy: 0.7450 - val_loss: 0.5698 - val_accuracy: 0.7270\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5277 - accuracy: 0.7800 - val_loss: 0.5426 - val_accuracy: 0.7685\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.5020 - accuracy: 0.8050 - val_loss: 0.5158 - val_accuracy: 0.8012\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.4726 - accuracy: 0.8300 - val_loss: 0.4921 - val_accuracy: 0.8200\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4488 - accuracy: 0.8550 - val_loss: 0.4697 - val_accuracy: 0.8398\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4261 - accuracy: 0.8850 - val_loss: 0.4506 - val_accuracy: 0.8576\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4062 - accuracy: 0.9100 - val_loss: 0.4328 - val_accuracy: 0.8694\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3881 - accuracy: 0.9200 - val_loss: 0.4169 - val_accuracy: 0.8783\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3720 - accuracy: 0.9250 - val_loss: 0.4025 - val_accuracy: 0.8892\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3573 - accuracy: 0.9300 - val_loss: 0.3894 - val_accuracy: 0.8981\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3436 - accuracy: 0.9350 - val_loss: 0.3768 - val_accuracy: 0.9011\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3314 - accuracy: 0.9300 - val_loss: 0.3655 - val_accuracy: 0.9031\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3201 - accuracy: 0.9400 - val_loss: 0.3548 - val_accuracy: 0.9041\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3087 - accuracy: 0.9400 - val_loss: 0.3452 - val_accuracy: 0.9080\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2994 - accuracy: 0.9500 - val_loss: 0.3363 - val_accuracy: 0.9080\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3565 - accuracy: 0.8935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.356483519077301, 0.8934999704360962]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – train and evaluate model B, without reusing model A\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model_B = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Create a new model based on model_A's layers\n",
    "model_A = tf.keras.models.load_model(\"my_model_A\")\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order not to affect model_A's layers during training model B_on_A, \n",
    "# we should clone model_B_on_A's architecture and then copy its weights\n",
    "# The layers will be different objects, even though they have the same names\n",
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.layers.reshaping.flatten.Flatten at 0x363636b60>, 'flatten_2')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.layers[0], model_A.layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.layers.reshaping.flatten.Flatten at 0x3663b58d0>, 'flatten_2')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A_clone.layers[0], model_A_clone.layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model_B_on_A again using the clone, replacing the top layer\n",
    "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the lower layers' weights to avoid wrecking them during the first few epochs of training\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 53ms/step - loss: 0.6417 - accuracy: 0.6350 - val_loss: 0.5300 - val_accuracy: 0.7092\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4842 - accuracy: 0.8100 - val_loss: 0.4981 - val_accuracy: 0.7745\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4622 - accuracy: 0.8250 - val_loss: 0.4850 - val_accuracy: 0.8061\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4436 - accuracy: 0.8500 - val_loss: 0.4677 - val_accuracy: 0.8170\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 51ms/step - loss: 0.4295 - accuracy: 0.8650 - val_loss: 0.4543 - val_accuracy: 0.8289\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4178 - accuracy: 0.8700 - val_loss: 0.4427 - val_accuracy: 0.8408\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.4084 - accuracy: 0.8800 - val_loss: 0.4331 - val_accuracy: 0.8457\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3958 - accuracy: 0.8850 - val_loss: 0.4208 - val_accuracy: 0.8536\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3850 - accuracy: 0.8900 - val_loss: 0.4110 - val_accuracy: 0.8576\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3781 - accuracy: 0.8900 - val_loss: 0.4014 - val_accuracy: 0.8665\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3688 - accuracy: 0.8950 - val_loss: 0.3949 - val_accuracy: 0.8625\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3635 - accuracy: 0.8900 - val_loss: 0.3856 - val_accuracy: 0.8675\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3528 - accuracy: 0.9000 - val_loss: 0.3772 - val_accuracy: 0.8783\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3458 - accuracy: 0.9000 - val_loss: 0.3702 - val_accuracy: 0.8863\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3392 - accuracy: 0.9100 - val_loss: 0.3623 - val_accuracy: 0.8942\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3320 - accuracy: 0.9050 - val_loss: 0.3557 - val_accuracy: 0.8961\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3252 - accuracy: 0.9100 - val_loss: 0.3496 - val_accuracy: 0.8961\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3203 - accuracy: 0.9100 - val_loss: 0.3443 - val_accuracy: 0.8991\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3149 - accuracy: 0.9150 - val_loss: 0.3385 - val_accuracy: 0.9001\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3099 - accuracy: 0.9150 - val_loss: 0.3332 - val_accuracy: 0.9011\n"
     ]
    }
   ],
   "source": [
    "# Train for a few epochs. Then unfreeze the lower layers and continue training with\n",
    "# a smaller learning rate\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.0005)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3522 - accuracy: 0.8855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35216501355171204, 0.8855000138282776]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, no improvement. But that's okay :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "2024-04-09 16:46:08.490508: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-04-09 16:46:08.490533: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-04-09 16:46:08.490541: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-04-09 16:46:08.490579: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-09 16:46:08.490600: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Momentum optimization\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Nesterov optimization\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "2024-04-09 17:23:53.945756: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-04-09 17:23:53.945776: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-04-09 17:23:53.945783: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-04-09 17:23:53.945837: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-09 17:23:53.946123: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# RMSProp\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power scheduling\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)  # decay= 1/s (s-> steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Scheduling\n",
    "def exponential_decay_fn(epoch):\n",
    "    # eta_0 = 0.01, s = 20\n",
    "    return 0.01 * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not hardcode eta_0 and s.\n",
    "# We do it in  this slightly complicated way because we have to supply the function\n",
    "# to the LearningRateScheduler callback, with no way to supply the extra arguments for lr0 and s\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally pass the current learning rate as a second argument to the schedule function\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (1 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scheduling\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Learning rate scheduling using a class from tf.keras.optimizers.schedules\n",
    "# Note that this approach updates the learning rate at each step rather than\n",
    "# at each epoch\n",
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 25\n",
    "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
    "scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01, decay_steps=n_steps, decay_rate=0.1\n",
    ")\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularization\n",
    "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use functools.partial to avoid passing the same arguments \n",
    "# again and again to every layer\n",
    "# Niceee\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(tf.keras.layers.Dense, activation=\"relu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   9/1719 [..............................] - ETA: 11s - loss: 2.4548 - accuracy: 0.2049 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 11:03:33.350395: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5830 - accuracy: 0.7879 - val_loss: 0.4248 - val_accuracy: 0.8382\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4518 - accuracy: 0.8355 - val_loss: 0.3971 - val_accuracy: 0.8494\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4205 - accuracy: 0.8464 - val_loss: 0.3807 - val_accuracy: 0.8584\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3974 - accuracy: 0.8543 - val_loss: 0.3506 - val_accuracy: 0.8724\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3808 - accuracy: 0.8601 - val_loss: 0.3645 - val_accuracy: 0.8652\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3707 - accuracy: 0.8630 - val_loss: 0.3558 - val_accuracy: 0.8660\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3615 - accuracy: 0.8669 - val_loss: 0.3469 - val_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3510 - accuracy: 0.8687 - val_loss: 0.3392 - val_accuracy: 0.8718\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3438 - accuracy: 0.8730 - val_loss: 0.3348 - val_accuracy: 0.8738\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3375 - accuracy: 0.8733 - val_loss: 0.3501 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "# extra code – compile and train the model\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3052 - accuracy: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.305176705121994, 0.8838363885879517]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The training accuracy looks like it's lower than the validation accuracy, \n",
    "# but that's just because dropout is only active during training. \n",
    "# If we evaluate the model on the training set after training (i.e., with dropout turned off), \n",
    "# we get the \"real\" training accuracy:\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC dropout: Sample 100 different models' predictions with dropout enabled\n",
    "# and take their average. This is generally more reliable than a single model's\n",
    "# prediction with dropout disabled\n",
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.092, 0.   ,\n",
       "        0.905]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:1]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.061, 0.   , 0.195, 0.   ,\n",
       "       0.744], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MC prediction is a little less confident\n",
    "y_proba[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 0.   , 0.   , 0.132, 0.   , 0.217, 0.002,\n",
       "       0.244], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much the predictions for the first sample vary across the different MC models\n",
    "y_std = y_probas.std(axis=0)\n",
    "y_std[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_proba.argmax(axis=1)\n",
    "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3792 - accuracy: 0.8624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37918326258659363, 0.8623999953269958]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have other layers that do something different during training and during inference,\n",
    "# we can't use training=True to implement MCDropout. Instead, we can subclass the Dropout class\n",
    "# to implement MCDropout.\n",
    "# Then, we can use it instead of Dropout layer when creating the model\n",
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs, training=False):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(1.)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson-ml3-DRIwlIAE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
