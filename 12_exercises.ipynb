{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "\n",
    "Implement a custom layer that performs layer normalization (we will use this type of layer in Chapter 15):\n",
    "\n",
    "1. The `build()` method should define two trainable weights **α** and **β**, both of shape `input_shape[-1:]` and data type `tf.float32`. **α** should be initialized with 1s, and **β** with 0s.\n",
    "\n",
    "2. The `call()` method should compute the mean _μ_ and standard deviation *σ* of each instance’s features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean *μ* and the variance *σ2* of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return **α** ⊗ (**X** – μ)/(σ + ε) + **β**, where ⊗ represents itemwise multiplication (`*`) and *ε* is a smoothing term (a small constant to avoid division by zero, e.g., 0.001).\n",
    "\n",
    "3. Ensure that your custom layer produces the same (or very nearly the same) output as the `tf.keras.layers.LayerNormalization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set TensorFlow to use CPU only\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset and split it to train, validation and test set\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 1) (11610, 1)\n"
     ]
    }
   ],
   "source": [
    "foo_mean, foo_var = tf.nn.moments(X_train_scaled, axes=-1, keepdims=True)\n",
    "print(foo_mean.shape, foo_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([11610, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sqrt(foo_var).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([11610, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.ones_like(foo_mean) * foo_mean).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = 1e-3\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.a = self.add_weight(\n",
    "            name=\"a\", shape=batch_input_shape[-1:], initializer=\"ones\"\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\", shape=batch_input_shape[-1:], initializer=\"zeros\"\n",
    "        )\n",
    "    \n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        # It is preferable to compute sqrt(variance + eps) instead of\n",
    "        # sqrt(variance) + eps because the derivative of sqrt(z) is\n",
    "        # undefined at z = 0 and training will bomp in this case\n",
    "        std = tf.sqrt(variance + self.eps)\n",
    "        return self.a * ((X - mean) / std) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([11610, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_normalization_layer = MyLayerNormalization()\n",
    "X_normalized = my_normalization_layer(X_train)\n",
    "X_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([11610, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_normalization_layer = tf.keras.layers.LayerNormalization()\n",
    "X_normalized_keras = keras_normalization_layer(X_train)\n",
    "X_normalized_keras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.3941788e-08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = tf.keras.metrics.MeanAbsoluteError()\n",
    "error(X_normalized_keras, X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there is not much difference between the two layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "Train a model using a custom training loop to tackle the Fashion MNIST dataset (see Chapter 10):\n",
    "\n",
    "1. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\n",
    "\n",
    "2. Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features as early as possible!\n",
    "X_train_full, X_test = X_train_full / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "for _ in range(5):\n",
    "    model.add(tf.keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119910 (468.40 KB)\n",
      "Trainable params: 119910 (468.40 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function that samples a batch of instances from the training set\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters\n",
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer_lower = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "optimizer_upper = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "val_metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "859/859 - mean_loss: 1.6184 - sparse_categorical_accuracy: 0.4681\n",
      "Validation loss: 0.9442396759986877, validation accuracy: 0.6852\n",
      "Epoch 2/5\n",
      "859/859 - mean_loss: 0.8209 - sparse_categorical_accuracy: 0.7198\n",
      "Validation loss: 0.7189589738845825, validation accuracy: 0.7576\n",
      "Epoch 3/5\n",
      "859/859 - mean_loss: 0.6954 - sparse_categorical_accuracy: 0.7587\n",
      "Validation loss: 0.6419775485992432, validation accuracy: 0.7688\n",
      "Epoch 4/5\n",
      "859/859 - mean_loss: 0.6288 - sparse_categorical_accuracy: 0.7751\n",
      "Validation loss: 0.5945408940315247, validation accuracy: 0.7812\n",
      "Epoch 5/5\n",
      "859/859 - mean_loss: 0.5926 - sparse_categorical_accuracy: 0.7831\n",
      "Validation loss: 0.5678561329841614, validation accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Build the custom training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer_lower.apply_gradients(zip(gradients[:-4], model.trainable_variables[:-4]))\n",
    "        optimizer_upper.apply_gradients(zip(gradients[-4:], model.trainable_variables[-4:]))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "    # Validation loss and accuracy\n",
    "    y_pred = model(X_valid)\n",
    "    val_loss = tf.reduce_mean(loss_fn(y_valid, y_pred))\n",
    "    for metric in val_metrics:\n",
    "        metric(y_valid, y_pred)\n",
    "    print(f\"Validation loss: {val_loss}, validation accuracy: {val_metrics[0].result():.4f}\")\n",
    "\n",
    "    for metric in [mean_loss] + metrics + val_metrics:\n",
    "        metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson-ml3-DRIwlIAE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
